{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from numbers import Number\n",
    "from tempfile import mkdtemp\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('ggplot')\n",
    "random_seed= 1618"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  A1     A2     A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15 A16\n",
      "0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  00202    0   +\n",
      "1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g  00043  560   +\n",
      "2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  00280  824   +\n",
      "3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  00100    3   +\n",
      "4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  00120    0   +\n"
     ]
    }
   ],
   "source": [
    "# Create column names according to data source: \"A1\" - \"A16\"\n",
    "col_names = [\"A\"+ str(i+1) for i in range(16)]\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('data/crx.data', header=None, names=col_names)\n",
    "\n",
    "# Inspect the first 10 rows\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               A3          A8        A11            A15\n",
      "count  690.000000  690.000000  690.00000     690.000000\n",
      "mean     4.758725    2.223406    2.40000    1017.385507\n",
      "std      4.978163    3.346513    4.86294    5210.102598\n",
      "min      0.000000    0.000000    0.00000       0.000000\n",
      "25%      1.000000    0.165000    0.00000       0.000000\n",
      "50%      2.750000    1.000000    0.00000       5.000000\n",
      "75%      7.207500    2.625000    3.00000     395.500000\n",
      "max     28.000000   28.500000   67.00000  100000.000000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   A1      690 non-null    object \n",
      " 1   A2      690 non-null    object \n",
      " 2   A3      690 non-null    float64\n",
      " 3   A4      690 non-null    object \n",
      " 4   A5      690 non-null    object \n",
      " 5   A6      690 non-null    object \n",
      " 6   A7      690 non-null    object \n",
      " 7   A8      690 non-null    float64\n",
      " 8   A9      690 non-null    object \n",
      " 9   A10     690 non-null    object \n",
      " 10  A11     690 non-null    int64  \n",
      " 11  A12     690 non-null    object \n",
      " 12  A13     690 non-null    object \n",
      " 13  A14     690 non-null    object \n",
      " 14  A15     690 non-null    int64  \n",
      " 15  A16     690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Inspect data\n",
    "print(data.describe())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reject    383\n",
      "accept    307\n",
      "Name: A16, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Change dependent varible labels to 1, 0\n",
    "data['A16'] = [1 if v == '+' else 0 for v in data['A16']]\n",
    "\n",
    "# Check dependent variable\n",
    "balance = data['A16'].value_counts().rename({0:'reject',1:'accept'})\n",
    "print(balance)\n",
    "\n",
    "# Notice A14 is zip code by seeing the values \n",
    "# Drop zip code \n",
    "data = data.drop(columns='A14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of '?'s: 54\n",
      "# of entries with nan: 31\n"
     ]
    }
   ],
   "source": [
    "# Print number of \"?\"\n",
    "print(f\"# of '?'s: {(data == '?').sum().sum()}\")\n",
    "\n",
    "# Replace '?' with nan\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "# Print number of entries containing missing values \n",
    "print(f\"# of entries with nan: {data.isna().any(axis=1).sum()}\")\n",
    "\n",
    "# Drop nan since columns are anonymous \n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column A1 into float type\n",
    "data['A2'] = data['A2'].astype('float')\n",
    "\n",
    "# Fix cat column \n",
    "cat_cols = data.select_dtypes(exclude=np.number).columns.to_list()\n",
    "for col in cat_cols:\n",
    "    data[col] = data[col].astype('category')\n",
    "\n",
    "X = data.drop(columns='A16')\n",
    "y = data['A16']\n",
    "\n",
    "# Split into train, test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=random_seed)\n",
    "num_cols = X.select_dtypes(include=np.number).columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function \n",
    "def estimator_name(estimator, local_items=locals().items()):\n",
    "    '''Get the variable name of an estimator by searching throught all local items. A helper function\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator: An estimator, i.e., LinearRegression()\n",
    "    string_2_unmatch: \n",
    "    local_items: A dictornary of all variabel items produced by local.items() in the local scale\n",
    "                where the estimators are\n",
    "    '''\n",
    "    for p, q in local_items:\n",
    "        # If match the estimator \n",
    "        if ((p !='estimator') and (q is estimator)):\n",
    "            return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding and min max scaling\n",
    "ohe_and_mmc = ColumnTransformer([('ohe',  OneHotEncoder(handle_unknown='ignore'), cat_cols),('mms', MinMaxScaler(), num_cols)]) \n",
    "\n",
    "# Use ROC AUC score as performamce measurement\n",
    "scorers = ['roc_auc']\n",
    "\n",
    "# Instanciate RepeatedStratifiedKFold\n",
    "n_repeats = 50\n",
    "n_splits = 10\n",
    "rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=random_seed)\n",
    "\n",
    "# Intanciate estimators\n",
    "logreg = LogisticRegression(random_state=random_seed)\n",
    "dt = DecisionTreeClassifier(max_depth=6, random_state=random_seed)\n",
    "rf = RandomForestClassifier(random_state=random_seed)\n",
    "gbc = GradientBoostingClassifier(random_state=random_seed)\n",
    "\n",
    "estimators = [logreg, dt, rf, gbc]\n",
    "estimator_names=[]\n",
    "cv_results = {}\n",
    "fit_all_X_train_results = {}\n",
    "local_items = locals().items()\n",
    "\n",
    "# Train models and store results \n",
    "for estimator in estimators:\n",
    "    # automate the naming process required by the Pipeline object\n",
    "    e_name = estimator_name(estimator, local_items)\n",
    "    estimator_names.append(e_name)\n",
    "    steps = [('ohe_and_mmc',ohe_and_mmc), (e_name, estimator)]\n",
    "    cachedir = mkdtemp()\n",
    "    pipe = Pipeline(steps, memory=cachedir)\n",
    "    cv_res = cross_validate(pipe, X_train, y_train, cv=rskf, return_train_score=True, return_estimator=True, scoring=scorers, error_score='raise', n_jobs=-1)   \n",
    "    cv_results[e_name] = cv_res\n",
    "\n",
    "    # Time the fitting process\n",
    "    start = time.perf_counter()\n",
    "    model_all_Xtrian = pipe.fit(X_train, y_train)\n",
    "    end = time.perf_counter()\n",
    "    time_fit_all_X_train = end - start\n",
    "\n",
    "    # Use fit_all_X_train_results to store all the results for models fitted with all train data\n",
    "    fit_all_X_train_results[e_name] = {'model': model_all_Xtrian}\n",
    "    fit_all_X_train_results[e_name]['fit_time'] = time_fit_all_X_train  \n",
    "    y_pred_proba = model_all_Xtrian.predict_proba(X_test)[:,1]\n",
    "    fit_all_X_train_results[e_name]['y_pred_proba'] = y_pred_proba\n",
    "    fit_all_X_train_results[e_name]['test_roc_auc'] = roc_auc_score(y_test, y_pred_proba)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "### 1. Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dicts for storing cross validation medians and means\n",
    "cv_medians = {}\n",
    "cv_means = {}\n",
    "\n",
    "for k, v in cv_results.items():\n",
    "    # Create a dicitonary for each estimator to store metrics\n",
    "    cv_medians[k] = {}\n",
    "    cv_means[k] = {}\n",
    "    for e_name, metric in v.items():\n",
    "        if e_name != 'estimator': \n",
    "            cv_medians[k][e_name]= np.median(metric)  \n",
    "            cv_means[k][e_name] = np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histplot for scores\n",
    "for e_name in estimator_names:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.hist(cv_results[e_name]['test_roc_auc'], bins=int(n_repeats*n_splits/3), alpha=0.5)\n",
    "    median = cv_medians[e_name]['test_roc_auc']\n",
    "    mean = cv_means[e_name]['test_roc_auc']\n",
    "    all_X_train = fit_all_X_train_results[e_name]['test_roc_auc']\n",
    "    plt.axvline(median, color='green', \n",
    "                label=f\"cv_median: {median:.3f}\")\n",
    "    plt.axvline(mean, color='red', \n",
    "                label=f\"cv_mean: {mean:.3f}\")\n",
    "    plt.axvline(all_X_train, color='yellow', \n",
    "                label=f\"all_X_train: {all_X_train:.3f}\")\n",
    "    plt.title('ROC AUC Score Historgam: '+ e_name)\n",
    "    plt.ylabel('Frquency')\n",
    "    plt.xlabel('ROC AUC Score')\n",
    "    plt.legend(loc='upper right')\n",
    "    filename = 'figures/histogram_'+e_name+'_.png'\n",
    "    plt.savefig(filename, facecolor='white')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function\n",
    "def position_tuples_from_boxplot(ax, showmeans=True):\n",
    "    '''Return a list of tuples of positions, like [(x, y_median, y_mean), ...] '''\n",
    "    res = []\n",
    "    if showmeans == True:\n",
    "        lines = ax.get_lines()\n",
    "        x_pos = ax.get_xticks()\n",
    "        for x in x_pos:\n",
    "            # every 4th line at the interval of 6 is the median line when showmeans = False;\n",
    "            # when showmeans = True, mean is the every 5th line at the interval of 7,\n",
    "            # (0 = 25th percentile, 1 = 75th percentile, 2 = lower whisker, \n",
    "            # 3 = upper whisker, 4 = 50th percentile (median), and 5 = mean, 6 = upper extreme value) \n",
    "            y_median = lines[4 + x*7].get_ydata()[0]\n",
    "            y_mean = lines[5 + x*7].get_ydata()[0]\n",
    "            res.append((x, y_median, y_mean))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame()\n",
    "for e_name in estimator_names:\n",
    "    score_df[e_name] = cv_results[e_name]['test_roc_auc']\n",
    "\n",
    "# Boxplot for scores\n",
    "facecolor = '#eaeaf2'\n",
    "fig_size = (10,8)\n",
    "h_padding = int(fig_size[0]*1.5)\n",
    "v_padding = int(fig_size[1]*1.5)\n",
    "title_size = int(fig_size[0]*1.8)\n",
    "xlabel_size = int(fig_size[1]*1.6)\n",
    "ylabel_size = int(fig_size[1]*1.6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=fig_size,facecolor=facecolor)\n",
    "bp = sns.boxplot(data=score_df,\n",
    "            showmeans=True,\n",
    "            meanprops={\"marker\": \".\",\n",
    "                    \"markeredgecolor\": \"red\",\n",
    "                    \"markerfacecolor\": \"red\",\n",
    "                    \"markersize\": \"10\"},\n",
    "            ax=ax, \n",
    "            palette='pastel')\n",
    "ax.tick_params(axis='x', labelsize=h_padding)\n",
    "\n",
    "# Plot a marker of the time for fitting all X train using a scatter plot\n",
    "pos = list(zip(*position_tuples_from_boxplot(bp)))\n",
    "x_pos_all_X_train = pos[0]\n",
    "y_pos_all_X_train = [v['test_roc_auc'] for _, v in fit_all_X_train_results.items()]\n",
    "plt.scatter(x_pos_all_X_train, y_pos_all_X_train, marker='*', color='blue', s=40)\n",
    "\n",
    "pos.append(tuple(y_pos_all_X_train))\n",
    "for x, ymedian, ymean, yalltrain in zip(*pos):\n",
    "    plt.annotate(text=f\"median: {ymedian:.3f}\", xy=(x, ymedian), xytext=(x, ymedian+0.02), color='green', \n",
    "                 ha = 'left', va = 'bottom', \n",
    "                 arrowprops=dict(color='green', arrowstyle='->'),)\n",
    "    plt.annotate(text=f\"mean: {ymean:.3f}\", xy=(x, ymean), xytext=(x, ymean+0.02), color='red', \n",
    "                 ha = 'right', va = 'top', \n",
    "                 arrowprops=dict(color='red', arrowstyle='->'),)\n",
    "    plt.annotate(text=f\"all_X_train: {yalltrain:.3f}\", xy=(x, yalltrain), xytext=(x, yalltrain+0.04), color='blue', \n",
    "                 ha = 'left', va = 'bottom', \n",
    "                 arrowprops=dict(color='blue', arrowstyle='->'),)\n",
    "\n",
    "plt.ylabel('ROC AUC Score', fontsize=ylabel_size, labelpad=h_padding)\n",
    "plt.xlabel('Estimator', fontsize=xlabel_size, labelpad=v_padding)\n",
    "plt.title('ROC AUC Boxplot', fontsize=title_size, pad=v_padding)\n",
    "plt.savefig('figures/boxplot_auc.png')\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_df = pd.DataFrame()\n",
    "for e_name in estimator_names:\n",
    "    time_df[e_name] = cv_results[e_name]['fit_time']\n",
    "# Score time is available with cv_results[name]['score_time'] \n",
    "\n",
    "# Boxplot for fit time\n",
    "fig_size = (10,8)\n",
    "h_padding = int(fig_size[0]*1.5)\n",
    "v_padding = int(fig_size[1]*1.5)\n",
    "title_size = int(fig_size[0]*1.8)\n",
    "xlabel_size = int(fig_size[1]*1.6)\n",
    "ylabel_size = int(fig_size[1]*1.6)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=fig_size,facecolor=facecolor)\n",
    "bp = sns.boxplot(data=time_df,\n",
    "            showmeans=True,\n",
    "            meanprops={\"marker\": \".\",\n",
    "                    \"markeredgecolor\": \"red\",\n",
    "                    \"markerfacecolor\": \"red\",\n",
    "                    \"markersize\": \"10\"},\n",
    "            ax=ax, \n",
    "            palette='pastel')\n",
    "ax.tick_params(axis='x', labelsize=int(fig_size[0]*1.5))\n",
    "\n",
    "# Plot a marker of the time for fitting all X train using a scatter plot\n",
    "pos = list(zip(*position_tuples_from_boxplot(bp)))\n",
    "x_pos_all_X_train = pos[0]\n",
    "y_pos_all_X_train = [v['fit_time'] for _, v in fit_all_X_train_results.items()]\n",
    "plt.scatter(x_pos_all_X_train, y_pos_all_X_train, marker='*', color='blue', s=40)\n",
    "\n",
    "pos.append(tuple(y_pos_all_X_train))\n",
    "for x, ymedian, ymean, yalltrain in zip(*pos):\n",
    "    plt.annotate(text=f\"median: {ymedian:.3f}\", xy=(x, ymedian), xytext=(x, ymedian+0.02), color='green', \n",
    "                 ha = 'left', va = 'bottom', \n",
    "                 arrowprops=dict(color='green', arrowstyle='->'),)\n",
    "    plt.annotate(text=f\"mean: {ymean:.3f}\", xy=(x, ymean), xytext=(x, ymean+0.02), color='red', \n",
    "                 ha = 'right', va = 'top', \n",
    "                 arrowprops=dict(color='red', arrowstyle='->'),)\n",
    "    plt.annotate(text=f\"all_X_train: {yalltrain:.3f}\", xy=(x, yalltrain), xytext=(x, yalltrain+0.04), color='blue', \n",
    "                 ha = 'left', va = 'bottom', \n",
    "                 arrowprops=dict(color='blue', arrowstyle='->'),)\n",
    "\n",
    "\n",
    "plt.ylabel('Time (secs)', fontsize=ylabel_size, labelpad=h_padding)\n",
    "plt.xlabel('Estimator', fontsize=xlabel_size, labelpad=v_padding)\n",
    "plt.title('Training Time Boxplot', fontsize=title_size, pad=v_padding) \n",
    "plt.savefig('figures/boxplot_time.png')\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "### 2. Test Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper function to plot roc auc curve\n",
    "def plot_roc_auc(y_true, y_pred_proba, fig_size=(8,8), title_string=f'ROC AUC Curve', save_as=None):\n",
    "    '''Plot ROC AUC curve'''\n",
    "    facecolor = 'white'\n",
    "    fig_size = (10,8)\n",
    "    h_padding = int(fig_size[0]*1.5)\n",
    "    v_padding = int(fig_size[1]*1.5)\n",
    "    title_size = int(fig_size[0]*1.8)\n",
    "    xlabel_size = int(fig_size[1]*1.6)\n",
    "    ylabel_size = int(fig_size[1]*1.6)\n",
    "    \n",
    "    # get fpr, tpr and threshold\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    fpr, tpr, threshold = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=fig_size, facecolor=facecolor)\n",
    "    # ROC line\n",
    "    plt.plot(fpr,tpr, label='ROC')\n",
    "    # AUC area\n",
    "    plt.fill_between(fpr, tpr, color='khaki', alpha=0.5, label=f'AUC: {auc:.3f}')\n",
    "\n",
    "    plt.legend(loc='upper left', fontsize='large', bbox_to_anchor=(1, 1))\n",
    "    plt.xlabel('False Positive Rate \\n (Type 1 Error)', size=xlabel_size, labelpad=h_padding)\n",
    "    # 'True Positive Rate = 1 - Type 2 Error Rate (a.k.a., Sensitivity or Recall or Power)'\n",
    "    plt.ylabel('True Positive Rate \\n (1 - Type 2 Error)',size=ylabel_size, labelpad=h_padding)\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(title_string, size=title_size, pad=v_padding)\n",
    "    ax.set_aspect('equal')\n",
    "    if save_as:\n",
    "        plt.savefig(save_as, bbox_inches='tight')\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot roc auc for models fitted with all training data\n",
    "for e_name in estimator_names:\n",
    "    plot_roc_auc(y_test, fit_all_X_train_results[e_name]['y_pred_proba'], \n",
    "                title_string=f'ROC AUC Curve: '+ e_name,\n",
    "                save_as='figures/roc_auc_curve_' + e_name + '.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a wrapper func to plot importances\n",
    "def plot_feature_importance(importance: pd.Series, top_n = 20, title = f\"Feature Importance / Coefficients\", \n",
    "                            fig_size=(15,8),\n",
    "                            save_as = f\"figures/importance_plot.png\"):\n",
    "    h_padding = int(fig_size[0]*1.5)\n",
    "    v_padding = int(fig_size[1]*1.5)\n",
    "    title_size = int(fig_size[0]*1.8)\n",
    "    xlabel_size = int(fig_size[1]*1.6)\n",
    "    ylabel_size = int(fig_size[1]*1.6)\n",
    "    if top_n > importance.shape[0]:\n",
    "        top_n = importance.shape[0]\n",
    "    importance = importance.iloc[:top_n]\n",
    "    x = [2*i for i in range(top_n)]\n",
    "    x_py = [i+0.3 for i in x]\n",
    "    fig, ax = plt.subplots(figsize=fig_size, facecolor='white')\n",
    "    plt.bar(x, importance.values)\n",
    "    plt.title(title, pad=v_padding)\n",
    "    plt.xticks(x, importance.index,rotation=60, ha='right')\n",
    "    for tk in plt.gca().get_xticklabels()[:5]:\n",
    "        tk.set_color('red')\n",
    "        tk.set_fontsize(15)\n",
    "    plt.xlabel(f\"Feature, Top {top_n}\", size=xlabel_size, labelpad=v_padding)\n",
    "    plt.ylabel(f\"Impotance / Coefficient\", size=ylabel_size, labelpad=h_padding)\n",
    "    plt.tight_layout()\n",
    "    if save_as:\n",
    "        plt.savefig(save_as)\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation (sort of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over estimator names and plot feature importance\n",
    "for e_name in estimator_names:\n",
    "    columns_bm = fit_all_X_train_results[e_name]['model'].named_steps['ohe_and_mmc'].get_feature_names_out()\n",
    "    try:\n",
    "        importance = fit_all_X_train_results[e_name]['model'].named_steps[e_name].coef_[0]\n",
    "    except:\n",
    "        importance = fit_all_X_train_results[e_name]['model'].named_steps[e_name].feature_importances_\n",
    "    importance = pd.Series(importance,index=columns_bm).sort_values(ascending=False, key=abs)\n",
    "\n",
    "    # plot importance \n",
    "    plot_feature_importance(importance, title=f\"Feature Importance / Coefficients: {e_name}\", \n",
    "                            save_as=f\"figures/importance_plot_{e_name}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclustion\n",
    "comment = f\"Pick the model with the best cross validation performances, \\\n",
    "since the training time doesn't vary significantly across all models. \"\n",
    "best_model = 'rf'\n",
    "most_importance_feature = f\"A9\"\n",
    "print(comment)\n",
    "print(f\"Best model: {fit_all_X_train_results[best_model]['model'][best_model]}.\")\n",
    "print(f\"The most importance feature: {most_importance_feature}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit_card_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
